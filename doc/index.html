<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="RAG Course Chatbot Documentation">
    <title>RAG Course Chatbot - Documentation</title>
    <style>
        /* Reset and base styles */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 0;
            margin: 0;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background-color: #ffffff;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
            min-height: 100vh;
        }
        
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem;
            text-align: center;
        }
        
        header h1 {
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
        }
        
        header p {
            font-size: 1.2rem;
            opacity: 0.9;
        }
        
        .content {
            padding: 2rem;
        }
        
        /* Typography */
        h1, h2, h3, h4, h5, h6 {
            color: #2c3e50;
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-weight: 600;
        }
        
        h1 {
            font-size: 2rem;
            border-bottom: 3px solid #667eea;
            padding-bottom: 0.5rem;
        }
        
        h2 {
            font-size: 1.75rem;
            border-bottom: 2px solid #e0e0e0;
            padding-bottom: 0.3rem;
            margin-top: 2.5rem;
        }
        
        h3 {
            font-size: 1.5rem;
            margin-top: 2rem;
        }
        
        h4 {
            font-size: 1.25rem;
            margin-top: 1.5rem;
        }
        
        /* Links */
        a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.2s;
        }
        
        a:hover {
            color: #764ba2;
            text-decoration: underline;
        }
        
        /* Lists */
        ul, ol {
            margin-left: 2rem;
            margin-bottom: 1rem;
        }
        
        li {
            margin-bottom: 0.5rem;
        }
        
        /* Code blocks */
        pre {
            background-color: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 1rem;
            overflow-x: auto;
            margin: 1rem 0;
        }
        
        code {
            font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', 'Consolas', monospace;
            font-size: 0.9em;
            background-color: #f8f9fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            color: #e83e8c;
        }
        
        pre code {
            background-color: transparent;
            padding: 0;
            color: #333;
        }
        
        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        th, td {
            padding: 0.75rem;
            text-align: left;
            border-bottom: 1px solid #e9ecef;
        }
        
        th {
            background-color: #667eea;
            color: white;
            font-weight: 600;
        }
        
        tr:hover {
            background-color: #f8f9fa;
        }
        
        /* Blockquotes */
        blockquote {
            border-left: 4px solid #667eea;
            padding-left: 1rem;
            margin: 1rem 0;
            color: #666;
            font-style: italic;
        }
        
        /* Horizontal rules */
        hr {
            border: none;
            border-top: 2px solid #e9ecef;
            margin: 2rem 0;
        }
        
        /* Checkboxes */
        input[type="checkbox"] {
            margin-right: 0.5rem;
        }
        
        /* Footer */
        footer {
            background-color: #2c3e50;
            color: white;
            padding: 1.5rem;
            text-align: center;
            margin-top: 3rem;
        }
        
        footer p {
            margin: 0.5rem 0;
        }
        
        .timestamp {
            font-size: 0.9em;
            opacity: 0.8;
        }
        
        /* Table of Contents */
        .toc {
            background-color: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 1.5rem;
            margin: 2rem 0;
        }
        
        .toc ul {
            list-style-type: none;
            margin-left: 0;
        }
        
        .toc li {
            margin-bottom: 0.5rem;
        }
        
        .toc a {
            color: #667eea;
        }
        
        /* Responsive design */
        @media (max-width: 768px) {
            .container {
                margin: 0;
            }
            
            header h1 {
                font-size: 1.8rem;
            }
            
            .content {
                padding: 1rem;
            }
            
            pre {
                font-size: 0.85em;
            }
        }
        
        /* Print styles */
        @media print {
            body {
                background-color: white;
            }
            
            .container {
                box-shadow: none;
            }
            
            header {
                background: #667eea !important;
                -webkit-print-color-adjust: exact;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>RAG Course Chatbot</h1>
            <p>Documentation</p>
        </header>
        
        <div class="content">
            <h2 id="rag-course-chatbot">RAG Course Chatbot<a class="headerlink" href="#rag-course-chatbot" title="Permanent link">&para;</a></h2>
<p>A Retrieval-Augmented Generation (RAG) chatbot that can answer questions based on your course content and documents.</p>
<h3 id="table-of-contents">Table of Contents<a class="headerlink" href="#table-of-contents" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="#quick-start">Quick Start</a></li>
<li><a href="#detailed-setup-guide">Detailed Setup Guide</a></li>
<li><a href="#1-installation">1. Installation</a></li>
<li><a href="#2-api-keys">2. API Keys</a></li>
<li><a href="#3-data-preparation">3. Data Preparation</a></li>
<li><a href="#4-how-to-execute-the-program">4. How to Execute the Program</a></li>
<li><a href="#5-configuration">5. Configuration</a></li>
<li><a href="#model-selection-and-priority">Model Selection and Priority</a></li>
<li><a href="#local-mode-no-api-keys-required">Local Mode (No API Keys Required)</a></li>
<li><a href="#additional-information">Additional Information</a></li>
<li><a href="#troubleshooting">Troubleshooting</a></li>
<li><a href="#quick-reference">Quick Reference</a></li>
</ul>
<h3 id="features">Features<a class="headerlink" href="#features" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Multi-Provider Support</strong>: OpenAI, Perplexity, Google Gemini (Copilot), HuggingFace</li>
<li><strong>Local LLM Support</strong>: Ollama, llama.cpp, and HuggingFace models optimized for macOS</li>
<li><strong>Apple Silicon Optimized</strong>: Automatic MPS (Metal) GPU acceleration on Apple Silicon Macs</li>
<li><strong>Automatic Fallbacks</strong>: If one provider fails, automatically tries the next</li>
<li><strong>Local Models</strong>: Run completely offline with multiple local LLM options</li>
<li><strong>Multiple File Types</strong>: PDF, DOCX, PPTX, TXT, URLs, YouTube videos</li>
<li><strong>Robust Error Handling</strong>: Graceful handling of corrupted files and API failures</li>
<li><strong>Modern LangChain API</strong>: Uses latest LangChain patterns with backward compatibility</li>
<li><strong>Code Optimization</strong>: Shared utilities eliminate code duplication</li>
<li><strong>Web Interface</strong>: Beautiful Streamlit interface for easy interaction</li>
</ul>
<h3 id="quick-start">Quick Start<a class="headerlink" href="#quick-start" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Install Python packages</strong> (see <a href="#1-installation">Installation</a> below)</li>
<li><strong>Configure directories</strong> in <code>config.yaml</code> (see <a href="#5-configuration">Configuration</a> below)</li>
<li><strong>Add your documents</strong> to the data directory (see <a href="#3-data-preparation">Data Preparation</a> below)</li>
<li><strong>Set up API keys</strong> (optional, see <a href="#2-api-keys">API Keys</a> below)</li>
<li><strong>Create the index</strong>: <code>python main.py</code></li>
<li><strong>Run the chatbot</strong>: <code>streamlit run rag_cli.py</code></li>
</ol>
<hr />
<h3 id="detailed-setup-guide">Detailed Setup Guide<a class="headerlink" href="#detailed-setup-guide" title="Permanent link">&para;</a></h3>
<h4 id="1-installation">1. Installation<a class="headerlink" href="#1-installation" title="Permanent link">&para;</a></h4>
<h5 id="system-requirements">System Requirements<a class="headerlink" href="#system-requirements" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>Python</strong>: 3.8 or higher</li>
<li><strong>RAM</strong>: 4GB minimum (8GB+ recommended for local models)</li>
<li><strong>Disk Space</strong>: 2-10GB (depending on models used)</li>
<li><strong>Operating System</strong>: macOS, Linux, or Windows</li>
</ul>
<h5 id="step-by-step-installation">Step-by-Step Installation<a class="headerlink" href="#step-by-step-installation" title="Permanent link">&para;</a></h5>
<ol>
<li>
<p><strong>Clone or download this repository</strong></p>
</li>
<li>
<p><strong>Create a virtual environment (recommended)</strong><br />
   ```bash<br />
   # Create virtual environment<br />
   python -m venv venv</p>
</li>
</ol>
<p># Activate virtual environment<br />
   # On macOS/Linux:<br />
   source venv/bin/activate<br />
   # On Windows:<br />
   venv\Scripts\activate<br />
   ```</p>
<ol start="3">
<li>
<p><strong>Install Python packages</strong><br />
<code>bash
   pip install -r requirements.txt</code></p>
</li>
<li>
<p><strong>Install optional dependencies for local LLMs</strong> (if using local models)</p>
</li>
</ol>
<p><strong>For Ollama:</strong><br />
<code>bash
   # Download and install from https://ollama.ai
   # Then pull a model:
   ollama pull llama2</code></p>
<p><strong>For llama.cpp:</strong><br />
<code>bash
   pip install llama-cpp-python</code></p>
<h5 id="verify-installation">Verify Installation<a class="headerlink" href="#verify-installation" title="Permanent link">&para;</a></h5>
<div class="highlight"><pre><span></span><code><span class="c1"># Check Python version</span>
python<span class="w"> </span>--version

<span class="c1"># Verify packages are installed</span>
pip<span class="w"> </span>list<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>langchain
</code></pre></div>

<hr />
<h4 id="2-api-keys">2. API Keys<a class="headerlink" href="#2-api-keys" title="Permanent link">&para;</a></h4>
<p>API keys are <strong>optional</strong> if you want to use local models. The system will automatically fall back to local models if API keys are not available.</p>
<h5 id="where-to-add-api-keys">Where to Add API Keys<a class="headerlink" href="#where-to-add-api-keys" title="Permanent link">&para;</a></h5>
<p>API keys should be set as <strong>environment variables</strong>. You can add them in several ways:</p>
<p><strong>Option 1: Export in terminal (temporary - lost when terminal closes)</strong></p>
<div class="highlight"><pre><span></span><code><span class="nb">export</span><span class="w"> </span><span class="nv">OPENAI_API_KEY</span><span class="o">=</span><span class="s2">&quot;your-openai-api-key&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">PERPLEXITY_API_KEY</span><span class="o">=</span><span class="s2">&quot;your-perplexity-api-key&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">GOOGLE_API_KEY</span><span class="o">=</span><span class="s2">&quot;your-google-api-key&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">HUGGINGFACE_API_KEY</span><span class="o">=</span><span class="s2">&quot;your-huggingface-api-key&quot;</span><span class="w">  </span><span class="c1"># Optional</span>
</code></pre></div>

<p><strong>Option 2: Create a <code>.env</code> file (recommended - persistent)</strong></p>
<p>Create a <code>.env</code> file in the project root directory:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Create .env file in the project root</span>
cat<span class="w"> </span>&gt;<span class="w"> </span>.env<span class="w"> </span><span class="s">&lt;&lt; EOF</span>
<span class="s">OPENAI_API_KEY=your-openai-api-key</span>
<span class="s">PERPLEXITY_API_KEY=your-perplexity-api-key</span>
<span class="s">GOOGLE_API_KEY=your-google-api-key</span>
<span class="s">HUGGINGFACE_API_KEY=your-huggingface-api-key</span>
<span class="s">LLAMA_CPP_MODEL_PATH=/path/to/your/model.gguf</span>
<span class="s">EOF</span>
</code></pre></div>

<p><strong>Example <code>.env</code> file:</strong></p>
<div class="highlight"><pre><span></span><code><span class="gh">#</span> OpenAI API Key (get from https://platform.openai.com/api-keys)
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxx

<span class="gh">#</span> Perplexity API Key (get from https://www.perplexity.ai/settings/api)
PERPLEXITY_API_KEY=pplx-xxxxxxxxxxxxxxxxxxxxx

<span class="gh">#</span> Google Gemini API Key (get from https://makersuite.google.com/app/apikey)
GOOGLE_API_KEY=AIzaSyxxxxxxxxxxxxxxxxxxxxx

<span class="gh">#</span> HuggingFace Token (optional, for private models)
HUGGINGFACE_API_KEY=hf_xxxxxxxxxxxxxxxxxxxxx

<span class="gh">#</span> llama.cpp Model Path (optional, for local GGUF models)
LLAMA_CPP_MODEL_PATH=~/.cache/llama-cpp/llama-2-7b-chat.gguf
</code></pre></div>

<p><strong>Important:</strong><br />
- The <code>.env</code> file is automatically loaded by the application (using <code>python-dotenv</code>)<br />
- <strong>Never commit <code>.env</code> to version control</strong> - it contains sensitive keys<br />
- Add <code>.env</code> to your <code>.gitignore</code> file:<br />
<code>bash
  echo ".env" &gt;&gt; .gitignore</code></p>
<p><strong>Option 3: Add to shell profile (permanent)</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Add to ~/.bashrc or ~/.zshrc</span>
<span class="nb">echo</span><span class="w"> </span><span class="s1">&#39;export OPENAI_API_KEY=&quot;your-openai-api-key&quot;&#39;</span><span class="w"> </span>&gt;&gt;<span class="w"> </span>~/.zshrc
<span class="nb">source</span><span class="w"> </span>~/.zshrc
</code></pre></div>

<h5 id="getting-api-keys">Getting API Keys<a class="headerlink" href="#getting-api-keys" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>OpenAI</strong>: https://platform.openai.com/api-keys</li>
<li><strong>Perplexity</strong>: https://www.perplexity.ai/settings/api</li>
<li><strong>Google Gemini</strong>: https://makersuite.google.com/app/apikey</li>
<li><strong>HuggingFace</strong>: https://huggingface.co/settings/tokens (optional, mainly for private models)</li>
</ul>
<h5 id="priority-order">Priority Order<a class="headerlink" href="#priority-order" title="Permanent link">&para;</a></h5>
<p>The system tries providers in this order (you can influence this by only setting certain API keys):</p>
<ol>
<li><strong>OpenAI</strong> (if <code>OPENAI_API_KEY</code> is set)</li>
<li><strong>Perplexity</strong> (if <code>PERPLEXITY_API_KEY</code> is set)</li>
<li><strong>Google Gemini</strong> (if <code>GOOGLE_API_KEY</code> is set)</li>
<li><strong>Ollama</strong> (if installed and models are available)</li>
<li><strong>llama.cpp</strong> (if <code>LLAMA_CPP_MODEL_PATH</code> is set)</li>
<li><strong>HuggingFace</strong> (local models, no API key needed)</li>
</ol>
<hr />
<h4 id="3-data-preparation">3. Data Preparation<a class="headerlink" href="#3-data-preparation" title="Permanent link">&para;</a></h4>
<h5 id="where-to-place-your-data">Where to Place Your Data<a class="headerlink" href="#where-to-place-your-data" title="Permanent link">&para;</a></h5>
<p>By default, documents should be placed in: <code>./rag-data/AI-Books/data/</code></p>
<p>You can change this location by editing <code>config.yaml</code> (see <a href="#5-configuration">Configuration</a> below).</p>
<h5 id="supported-file-formats">Supported File Formats<a class="headerlink" href="#supported-file-formats" title="Permanent link">&para;</a></h5>
<p>The system supports the following file types:</p>
<table>
<thead>
<tr>
<th>Format</th>
<th>Extension</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>PDF</td>
<td><code>.pdf</code></td>
<td>PDF documents</td>
</tr>
<tr>
<td>Word</td>
<td><code>.docx</code></td>
<td>Microsoft Word documents</td>
</tr>
<tr>
<td>PowerPoint</td>
<td><code>.pptx</code></td>
<td>Microsoft PowerPoint presentations</td>
</tr>
<tr>
<td>Text</td>
<td><code>.txt</code></td>
<td>Plain text files</td>
</tr>
<tr>
<td>URL</td>
<td><code>.url</code></td>
<td>Text file containing a web URL (one URL per line)</td>
</tr>
<tr>
<td>YouTube</td>
<td><code>.youtube</code></td>
<td>Text file containing a YouTube URL (one URL per line)</td>
</tr>
</tbody>
</table>
<h5 id="data-directory-structure">Data Directory Structure<a class="headerlink" href="#data-directory-structure" title="Permanent link">&para;</a></h5>
<div class="highlight"><pre><span></span><code>rag-data/
└── AI-Books/
    ├── data/           # Place your source documents here
    │   ├── document1.pdf
    │   ├── document2.docx
    │   ├── notes.txt
    │   ├── website.url
    │   └── video.youtube
    └── index/          # Generated index (created automatically)
        ├── index.faiss
        └── index.pkl
</code></pre></div>

<h5 id="file-format-examples">File Format Examples<a class="headerlink" href="#file-format-examples" title="Permanent link">&para;</a></h5>
<p><strong>For URL files (<code>.url</code>):</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Create a file named &quot;example.url&quot; with content:</span>
https://example.com/article
</code></pre></div>

<p><strong>For YouTube files (<code>.youtube</code>):</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Create a file named &quot;tutorial.youtube&quot; with content:</span>
https://www.youtube.com/watch?v<span class="o">=</span>VIDEO_ID
</code></pre></div>

<h5 id="best-practices">Best Practices<a class="headerlink" href="#best-practices" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>Organize files</strong>: Use subdirectories to organize your documents</li>
<li><strong>File naming</strong>: Use descriptive names for easier identification</li>
<li><strong>File size</strong>: Large files (&gt;100MB) may take longer to process</li>
<li><strong>Corrupted files</strong>: The system automatically skips corrupted PDFs</li>
<li><strong>Excluded files</strong>: Files in <code>.git</code> directories are automatically skipped</li>
</ul>
<hr />
<h4 id="4-how-to-execute-the-program">4. How to Execute the Program<a class="headerlink" href="#4-how-to-execute-the-program" title="Permanent link">&para;</a></h4>
<h5 id="step-1-create-the-index">Step 1: Create the Index<a class="headerlink" href="#step-1-create-the-index" title="Permanent link">&para;</a></h5>
<p>Before running the chatbot, you need to create an index of your documents:</p>
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>main.py
</code></pre></div>

<p><strong>What this does:</strong><br />
- Scans the data directory for supported files<br />
- Loads and processes all documents<br />
- Creates embeddings using the best available provider<br />
- Builds a searchable vector index<br />
- Saves the index to the index directory<br />
- Provides detailed logging of the process</p>
<p><strong>Expected output:</strong></p>
<div class="highlight"><pre><span></span><code><span class="n">INFO</span><span class="p">:</span><span class="n">__main__</span><span class="p">:</span><span class="n">Loaded</span><span class="w"> </span><span class="mi">25</span><span class="w"> </span><span class="n">documents</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">indexing</span><span class="o">.</span>
<span class="n">INFO</span><span class="p">:</span><span class="n">__main__</span><span class="p">:</span><span class="n">Successfully</span><span class="w"> </span><span class="n">loaded</span><span class="w"> </span><span class="n">OpenAI</span><span class="w"> </span><span class="n">embeddings</span>
<span class="n">INFO</span><span class="p">:</span><span class="n">__main__</span><span class="p">:</span><span class="n">Indexing</span><span class="w"> </span><span class="n">complete</span><span class="o">.</span>
</code></pre></div>

<p><strong>If you see errors:</strong><br />
- Check that your data directory contains supported files<br />
- Verify API keys are set (if using cloud providers)<br />
- Ensure sufficient disk space for the index</p>
<h5 id="step-2-run-the-chatbot">Step 2: Run the Chatbot<a class="headerlink" href="#step-2-run-the-chatbot" title="Permanent link">&para;</a></h5>
<p>After creating the index, start the web interface:</p>
<div class="highlight"><pre><span></span><code>streamlit<span class="w"> </span>run<span class="w"> </span>rag_cli.py
</code></pre></div>

<p><strong>What happens:</strong><br />
- Streamlit server starts<br />
- Opens a web browser automatically<br />
- Loads the RAG chain with your index<br />
- Displays the chatbot interface</p>
<p><strong>Access the interface:</strong><br />
- The app will open automatically in your browser<br />
- Or navigate to: <code>http://localhost:8501</code></p>
<p><strong>Using the chatbot:</strong><br />
1. Type your question in the input box<br />
2. Click Enter or wait for processing<br />
3. View the answer and source documents<br />
4. Continue the conversation (chat history is maintained)</p>
<h5 id="re-indexing">Re-indexing<a class="headerlink" href="#re-indexing" title="Permanent link">&para;</a></h5>
<p>If you add new documents, re-run the indexing:</p>
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>main.py
</code></pre></div>

<p>The old index will be overwritten with the new one.</p>
<hr />
<h4 id="5-configuration">5. Configuration<a class="headerlink" href="#5-configuration" title="Permanent link">&para;</a></h4>
<h5 id="configuring-directories">Configuring Directories<a class="headerlink" href="#configuring-directories" title="Permanent link">&para;</a></h5>
<p>Edit <code>config.yaml</code> to change data and index locations:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># config.yaml</span>
<span class="nt">DATA_DIR</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;./rag-data/AI-Books/data&quot;</span><span class="w">    </span><span class="c1"># Source documents directory</span>
<span class="nt">INDEX_DIR</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;./rag-data/AI-Books/index&quot;</span><span class="w">   </span><span class="c1"># Generated index directory</span>
</code></pre></div>

<p><strong>Example: Using custom directories</strong></p>
<div class="highlight"><pre><span></span><code><span class="nt">DATA_DIR</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;/Users/yourname/Documents/my-docs&quot;</span>
<span class="nt">INDEX_DIR</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;/Users/yourname/Documents/my-index&quot;</span>
</code></pre></div>

<p><strong>Important:</strong><br />
- Use absolute paths for reliability<br />
- Ensure the index directory exists or can be created<br />
- The index directory will be created automatically if it doesn't exist</p>
<h5 id="changing-model-priority">Changing Model Priority<a class="headerlink" href="#changing-model-priority" title="Permanent link">&para;</a></h5>
<p>The system automatically selects models based on availability. To force a specific model:</p>
<p><strong>Method 1: Set only the desired API key</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Only use OpenAI (remove other API keys)</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OPENAI_API_KEY</span><span class="o">=</span><span class="s2">&quot;your-key&quot;</span>
<span class="c1"># Unset others</span>
<span class="nb">unset</span><span class="w"> </span>PERPLEXITY_API_KEY
<span class="nb">unset</span><span class="w"> </span>GOOGLE_API_KEY
</code></pre></div>

<p><strong>Method 2: Modify <code>utils.py</code></strong><br />
Edit the <code>get_llm_model()</code> function in <code>utils.py</code> to change the order of model attempts.</p>
<p><strong>Method 3: Use environment variables for local models</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Force llama.cpp</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LLAMA_CPP_MODEL_PATH</span><span class="o">=</span><span class="s2">&quot;/path/to/your/model.gguf&quot;</span>

<span class="c1"># Force Ollama (ensure Ollama is running)</span>
<span class="c1"># The system will automatically detect available Ollama models</span>
</code></pre></div>

<h5 id="model-selection-examples">Model Selection Examples<a class="headerlink" href="#model-selection-examples" title="Permanent link">&para;</a></h5>
<p><strong>Use only local models (no API keys):</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Don&#39;t set any API keys</span>
<span class="c1"># Install Ollama and pull a model:</span>
ollama<span class="w"> </span>pull<span class="w"> </span>llama2
<span class="c1"># Or set llama.cpp path:</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LLAMA_CPP_MODEL_PATH</span><span class="o">=</span><span class="s2">&quot;~/.cache/llama-cpp/model.gguf&quot;</span>
</code></pre></div>

<p><strong>Use only OpenAI:</strong></p>
<div class="highlight"><pre><span></span><code><span class="nb">export</span><span class="w"> </span><span class="nv">OPENAI_API_KEY</span><span class="o">=</span><span class="s2">&quot;your-key&quot;</span>
<span class="c1"># Don&#39;t set other API keys</span>
</code></pre></div>

<p><strong>Use Perplexity as primary, fallback to local:</strong></p>
<div class="highlight"><pre><span></span><code><span class="nb">export</span><span class="w"> </span><span class="nv">PERPLEXITY_API_KEY</span><span class="o">=</span><span class="s2">&quot;your-key&quot;</span>
<span class="c1"># Don&#39;t set OpenAI or Google keys</span>
<span class="c1"># Local models will be used if Perplexity fails</span>
</code></pre></div>

<hr />
<h3 id="model-selection-and-priority">Model Selection and Priority<a class="headerlink" href="#model-selection-and-priority" title="Permanent link">&para;</a></h3>
<h4 id="how-models-are-selected">How Models Are Selected<a class="headerlink" href="#how-models-are-selected" title="Permanent link">&para;</a></h4>
<p>The system automatically selects the best available model based on:<br />
1. <strong>API keys set</strong> (cloud providers are tried first)<br />
2. <strong>Model availability</strong> (local models are tried if cloud fails)<br />
3. <strong>Performance</strong> (faster models are preferred)</p>
<h4 id="embedding-models-priority-order">Embedding Models (Priority Order)<a class="headerlink" href="#embedding-models-priority-order" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>OpenAI</strong> (<code>text-embedding-ada-002</code>) - Requires <code>OPENAI_API_KEY</code></li>
<li><strong>Perplexity</strong> (built-in embeddings) - Requires <code>PERPLEXITY_API_KEY</code></li>
<li><strong>Google Gemini</strong> (<code>embedding-001</code>) - Requires <code>GOOGLE_API_KEY</code></li>
<li><strong>HuggingFace BGE</strong> (<code>BAAI/bge-small-en-v1.5</code>) - Local, high quality</li>
<li><strong>HuggingFace Sentence Transformers</strong> (<code>all-MiniLM-L6-v2</code>) - Local fallback</li>
</ol>
<h4 id="llm-models-priority-order">LLM Models (Priority Order)<a class="headerlink" href="#llm-models-priority-order" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>OpenAI</strong> (<code>gpt-3.5-turbo</code>) - Requires <code>OPENAI_API_KEY</code></li>
<li><strong>Perplexity</strong> (<code>mistral-7b-instruct</code>) - Requires <code>PERPLEXITY_API_KEY</code></li>
<li><strong>Google Gemini</strong> (<code>gemini-pro</code>) - Requires <code>GOOGLE_API_KEY</code></li>
<li><strong>Ollama</strong> (<code>llama2</code>, <code>mistral</code>, <code>llama3</code>, <code>phi3</code>, <code>gemma</code>) - Local, requires Ollama installation</li>
<li><strong>llama.cpp</strong> (GGUF models) - Local, requires <code>LLAMA_CPP_MODEL_PATH</code></li>
<li><strong>HuggingFace</strong> (<code>phi-2</code>, <code>TinyLlama</code>, <code>DialoGPT-medium</code>, <code>GPT-2</code>, <code>DistilGPT-2</code>) - Local, automatic</li>
</ol>
<h4 id="changing-models">Changing Models<a class="headerlink" href="#changing-models" title="Permanent link">&para;</a></h4>
<p>See <a href="#5-configuration">Configuration</a> section above for detailed instructions.</p>
<h3 id="local-mode-no-api-keys-required">Local Mode (No API Keys Required)<a class="headerlink" href="#local-mode-no-api-keys-required" title="Permanent link">&para;</a></h3>
<p>The system can run completely offline using local models. Multiple options are available:</p>
<h4 id="option-1-ollama-recommended-for-macos">Option 1: Ollama (Recommended for macOS)<a class="headerlink" href="#option-1-ollama-recommended-for-macos" title="Permanent link">&para;</a></h4>
<p>Ollama is the easiest way to run local LLMs on macOS:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Install Ollama from https://ollama.ai</span>
<span class="c1"># Then pull a model:</span>
ollama<span class="w"> </span>pull<span class="w"> </span>llama2
<span class="c1"># or</span>
ollama<span class="w"> </span>pull<span class="w"> </span>mistral

<span class="c1"># Run the RAG system</span>
python<span class="w"> </span>main.py
streamlit<span class="w"> </span>run<span class="w"> </span>rag_cli.py
</code></pre></div>

<p>The system will automatically detect and use available Ollama models.</p>
<h4 id="option-2-llamacpp-optimized-for-macos">Option 2: llama.cpp (Optimized for macOS)<a class="headerlink" href="#option-2-llamacpp-optimized-for-macos" title="Permanent link">&para;</a></h4>
<p>For GGUF models optimized for Apple Silicon:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Install llama-cpp-python</span>
pip<span class="w"> </span>install<span class="w"> </span>llama-cpp-python

<span class="c1"># Download a GGUF model (e.g., from HuggingFace)</span>
<span class="c1"># Set the model path:</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LLAMA_CPP_MODEL_PATH</span><span class="o">=</span>~/.cache/llama-cpp/llama-2-7b-chat.gguf

<span class="c1"># Run the RAG system</span>
python<span class="w"> </span>main.py
streamlit<span class="w"> </span>run<span class="w"> </span>rag_cli.py
</code></pre></div>

<h4 id="option-3-huggingface-models-apple-silicon-optimized">Option 3: HuggingFace Models (Apple Silicon Optimized)<a class="headerlink" href="#option-3-huggingface-models-apple-silicon-optimized" title="Permanent link">&para;</a></h4>
<p>The system automatically uses Apple Silicon GPU (MPS) when available:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># No API keys needed - uses local models</span>
python<span class="w"> </span>main.py
streamlit<span class="w"> </span>run<span class="w"> </span>rag_cli.py
</code></pre></div>

<p>This will use:<br />
- <strong>Embeddings</strong>: HuggingFace sentence-transformers (local)<br />
- <strong>LLM</strong>: HuggingFace models optimized for Apple Silicon (phi-2, TinyLlama, etc.)</p>
<h3 id="error-handling">Error Handling<a class="headerlink" href="#error-handling" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Corrupted PDFs</strong>: Automatically detected and skipped</li>
<li><strong>API Quota Exceeded</strong>: Automatic fallback to next provider</li>
<li><strong>Network Issues</strong>: Graceful error messages and retry logic</li>
<li><strong>Missing Dependencies</strong>: Clear installation instructions</li>
<li><strong>Local Model Failures</strong>: Multiple model fallbacks</li>
</ul>
<hr />
<h3 id="additional-information">Additional Information<a class="headerlink" href="#additional-information" title="Permanent link">&para;</a></h3>
<h4 id="workflow-overview">Workflow Overview<a class="headerlink" href="#workflow-overview" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="mf">1.</span><span class="w"> </span><span class="n">Install</span><span class="w"> </span><span class="n">packages</span>
<span class="w">   </span><span class="err">↓</span>
<span class="mf">2.</span><span class="w"> </span><span class="n">Configure</span><span class="w"> </span><span class="n">directories</span><span class="w"> </span><span class="p">(</span><span class="n">config</span><span class="mf">.</span><span class="n">yaml</span><span class="p">)</span>
<span class="w">   </span><span class="err">↓</span>
<span class="mf">3.</span><span class="w"> </span><span class="n">Add</span><span class="w"> </span><span class="n">documents</span><span class="w"> </span><span class="kr">to</span><span class="w"> </span><span class="kd">data</span><span class="w"> </span><span class="n">directory</span>
<span class="w">   </span><span class="err">↓</span>
<span class="mf">4.</span><span class="w"> </span><span class="n">Set</span><span class="w"> </span><span class="n">API</span><span class="w"> </span><span class="n">keys</span><span class="w"> </span><span class="p">(</span><span class="n">optional</span><span class="p">)</span>
<span class="w">   </span><span class="err">↓</span>
<span class="mf">5.</span><span class="w"> </span><span class="kr">Run</span><span class="p">:</span><span class="w"> </span><span class="n">python</span><span class="w"> </span><span class="n">main</span><span class="mf">.</span><span class="n">py</span><span class="w"> </span><span class="p">(</span><span class="n">creates</span><span class="w"> </span><span class="n">index</span><span class="p">)</span>
<span class="w">   </span><span class="err">↓</span>
<span class="mf">6.</span><span class="w"> </span><span class="kr">Run</span><span class="p">:</span><span class="w"> </span><span class="n">streamlit</span><span class="w"> </span><span class="kr">run</span><span class="w"> </span><span class="n">rag_cli</span><span class="mf">.</span><span class="n">py</span><span class="w"> </span><span class="p">(</span><span class="n">starts</span><span class="w"> </span><span class="n">chatbot</span><span class="p">)</span>
<span class="w">   </span><span class="err">↓</span>
<span class="mf">7.</span><span class="w"> </span><span class="n">Ask</span><span class="w"> </span><span class="n">questions</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">web</span><span class="w"> </span><span class="nb">int</span><span class="n">erface</span>
</code></pre></div>

<h4 id="environment-variables-reference">Environment Variables Reference<a class="headerlink" href="#environment-variables-reference" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>OPENAI_API_KEY</code></td>
<td>No</td>
<td>OpenAI API key for embeddings and LLM</td>
</tr>
<tr>
<td><code>PERPLEXITY_API_KEY</code></td>
<td>No</td>
<td>Perplexity API key for embeddings and LLM</td>
</tr>
<tr>
<td><code>GOOGLE_API_KEY</code></td>
<td>No</td>
<td>Google Gemini API key for embeddings and LLM</td>
</tr>
<tr>
<td><code>HUGGINGFACE_API_KEY</code></td>
<td>No</td>
<td>HuggingFace token (optional, for private models)</td>
</tr>
<tr>
<td><code>LLAMA_CPP_MODEL_PATH</code></td>
<td>No</td>
<td>Path to GGUF model file for llama.cpp</td>
</tr>
<tr>
<td><code>USER_AGENT</code></td>
<td>No</td>
<td>User agent string (default: "rag-chatbot/1.0")</td>
</tr>
</tbody>
</table>
<h4 id="file-structure-details">File Structure Details<a class="headerlink" href="#file-structure-details" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="n">rag</span><span class="o">/</span>
<span class="err">├──</span><span class="w"> </span><span class="n">main</span><span class="o">.</span><span class="n">py</span><span class="w">              </span><span class="c1"># Indexing script - creates vector index</span>
<span class="err">├──</span><span class="w"> </span><span class="n">rag_cli</span><span class="o">.</span><span class="n">py</span><span class="w">           </span><span class="c1"># Streamlit chatbot interface</span>
<span class="err">├──</span><span class="w"> </span><span class="n">utils</span><span class="o">.</span><span class="n">py</span><span class="w">             </span><span class="c1"># Shared utilities for embeddings and LLMs</span>
<span class="err">├──</span><span class="w"> </span><span class="n">generate_docs</span><span class="o">.</span><span class="n">py</span><span class="w">     </span><span class="c1"># Script to convert README.md to HTML</span>
<span class="err">├──</span><span class="w"> </span><span class="n">config</span><span class="o">.</span><span class="n">yaml</span><span class="w">          </span><span class="c1"># Configuration file (data/index directories)</span>
<span class="err">├──</span><span class="w"> </span><span class="n">requirements</span><span class="o">.</span><span class="n">txt</span><span class="w">     </span><span class="c1"># Python package dependencies</span>
<span class="err">├──</span><span class="w"> </span><span class="o">.</span><span class="n">env</span><span class="w">                 </span><span class="c1"># Environment variables (create this file)</span>
<span class="err">├──</span><span class="w"> </span><span class="n">doc</span><span class="o">/</span><span class="w">                 </span><span class="c1"># HTML documentation</span>
<span class="err">│</span><span class="w">   </span><span class="err">├──</span><span class="w"> </span><span class="n">index</span><span class="o">.</span><span class="n">html</span><span class="w">      </span><span class="c1"># HTML version of this README</span>
<span class="err">│</span><span class="w">   </span><span class="err">└──</span><span class="w"> </span><span class="n">README</span><span class="o">.</span><span class="n">md</span><span class="w">       </span><span class="c1"># Documentation about the HTML docs</span>
<span class="err">├──</span><span class="w"> </span><span class="n">rag</span><span class="o">-</span><span class="n">data</span><span class="o">/</span><span class="w">           </span><span class="c1"># Your documents directory</span>
<span class="err">│</span><span class="w">   </span><span class="err">└──</span><span class="w"> </span><span class="n">AI</span><span class="o">-</span><span class="n">Books</span><span class="o">/</span>
<span class="err">│</span><span class="w">       </span><span class="err">├──</span><span class="w"> </span><span class="n">data</span><span class="o">/</span><span class="w">       </span><span class="c1"># Source documents (place files here)</span>
<span class="err">│</span><span class="w">       </span><span class="err">└──</span><span class="w"> </span><span class="n">index</span><span class="o">/</span><span class="w">      </span><span class="c1"># Generated index (created by main.py)</span>
<span class="err">│</span><span class="w">           </span><span class="err">├──</span><span class="w"> </span><span class="n">index</span><span class="o">.</span><span class="n">faiss</span>
<span class="err">│</span><span class="w">           </span><span class="err">└──</span><span class="w"> </span><span class="n">index</span><span class="o">.</span><span class="n">pkl</span>
<span class="err">└──</span><span class="w"> </span><span class="n">README</span><span class="o">.</span><span class="n">md</span><span class="w">           </span><span class="c1"># This documentation</span>
</code></pre></div>

<h4 id="html-documentation">HTML Documentation<a class="headerlink" href="#html-documentation" title="Permanent link">&para;</a></h4>
<p>An HTML version of this documentation is available in the <code>doc/</code> folder. It's automatically generated from <code>README.md</code> and can be viewed in any web browser.</p>
<p><strong>To generate/update the HTML documentation:</strong></p>
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>generate_docs.py
</code></pre></div>

<p><strong>To view the HTML documentation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Open in browser</span>
open<span class="w"> </span>doc/index.html<span class="w">  </span><span class="c1"># macOS</span>
<span class="c1"># or</span>
xdg-open<span class="w"> </span>doc/index.html<span class="w">  </span><span class="c1"># Linux</span>
<span class="c1"># or</span>
start<span class="w"> </span>doc/index.html<span class="w">  </span><span class="c1"># Windows</span>

<span class="c1"># Or serve with HTTP server</span>
python<span class="w"> </span>-m<span class="w"> </span>http.server<span class="w"> </span><span class="m">8000</span><span class="w"> </span>--directory<span class="w"> </span>doc
<span class="c1"># Then visit http://localhost:8000</span>
</code></pre></div>

<p><strong>Auto-update on README.md changes:</strong><br />
See <code>doc/README.md</code> for instructions on setting up automatic updates via git hooks or file watchers.</p>
<h4 id="understanding-the-index">Understanding the Index<a class="headerlink" href="#understanding-the-index" title="Permanent link">&para;</a></h4>
<p>The index is a vector database that stores:<br />
- <strong>Document chunks</strong>: Text split into manageable pieces<br />
- <strong>Embeddings</strong>: Vector representations of text chunks<br />
- <strong>Metadata</strong>: Source file information</p>
<p><strong>Index files:</strong><br />
- <code>index.faiss</code>: FAISS vector index (binary)<br />
- <code>index.pkl</code>: Metadata and configuration (pickle)</p>
<p><strong>When to re-index:</strong><br />
- After adding new documents<br />
- After modifying existing documents<br />
- If the index becomes corrupted</p>
<h4 id="performance-optimization">Performance Optimization<a class="headerlink" href="#performance-optimization" title="Permanent link">&para;</a></h4>
<p><strong>For faster indexing:</strong><br />
- Use cloud embeddings (OpenAI, Perplexity) instead of local<br />
- Reduce number of documents<br />
- Use smaller chunk sizes (edit <code>main.py</code>)</p>
<p><strong>For faster queries:</strong><br />
- Use cloud LLMs (OpenAI, Perplexity) for faster responses<br />
- Reduce <code>k</code> value in retriever (fewer documents retrieved)<br />
- Use smaller local models</p>
<p><strong>For local models:</strong><br />
- Use Ollama for best performance on macOS<br />
- Ensure sufficient RAM (8GB+ recommended)<br />
- Use Apple Silicon for GPU acceleration</p>
<hr />
<h3 id="troubleshooting">Troubleshooting<a class="headerlink" href="#troubleshooting" title="Permanent link">&para;</a></h3>
<h4 id="common-issues-and-solutions">Common Issues and Solutions<a class="headerlink" href="#common-issues-and-solutions" title="Permanent link">&para;</a></h4>
<h5 id="1-no-documents-found-to-index">1. "No documents found to index"<a class="headerlink" href="#1-no-documents-found-to-index" title="Permanent link">&para;</a></h5>
<p><strong>Problem:</strong> The indexing script can't find any documents.</p>
<p><strong>Solutions:</strong><br />
- Check that <code>DATA_DIR</code> in <code>config.yaml</code> points to the correct directory<br />
- Verify files have supported extensions (<code>.pdf</code>, <code>.docx</code>, <code>.pptx</code>, <code>.txt</code>, <code>.url</code>, <code>.youtube</code>)<br />
- Ensure files are not in <code>.git</code> directories (automatically skipped)<br />
- Check file permissions (ensure files are readable)</p>
<p><strong>Debug:</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Check if files are in the directory</span>
ls<span class="w"> </span>-la<span class="w"> </span>rag-data/AI-Books/data/

<span class="c1"># Verify config.yaml path</span>
cat<span class="w"> </span>config.yaml
</code></pre></div>

<h5 id="2-all-llm-providers-failed">2. "All LLM providers failed"<a class="headerlink" href="#2-all-llm-providers-failed" title="Permanent link">&para;</a></h5>
<p><strong>Problem:</strong> No LLM provider is available or working.</p>
<p><strong>Solutions:</strong><br />
- <strong>For cloud providers:</strong> Verify API keys are set correctly<br />
<code>bash
  echo $OPENAI_API_KEY  # Should show your key</code><br />
- <strong>For local models:</strong> Ensure models are installed<br />
  ```bash<br />
  # Check Ollama<br />
  ollama list</p>
<p># Check llama.cpp<br />
  echo $LLAMA_CPP_MODEL_PATH<br />
  ```<br />
- Check internet connection (needed for cloud providers and model downloads)<br />
- Verify API quota hasn't been exceeded<br />
- Try local mode (don't set any API keys)</p>
<h5 id="3-failed-to-load-the-rag-chain">3. "Failed to load the RAG chain"<a class="headerlink" href="#3-failed-to-load-the-rag-chain" title="Permanent link">&para;</a></h5>
<p><strong>Problem:</strong> The chatbot can't load the index or models.</p>
<p><strong>Solutions:</strong><br />
- <strong>Run indexing first:</strong> <code>python main.py</code><br />
- Check that index files exist:<br />
<code>bash
  ls -la rag-data/AI-Books/index/</code><br />
- Verify <code>INDEX_DIR</code> in <code>config.yaml</code> is correct<br />
- Check file permissions on index directory<br />
- Re-create the index if corrupted:<br />
<code>bash
  rm -rf rag-data/AI-Books/index/*
  python main.py</code></p>
<h5 id="4-huggingface-models-failed-to-load">4. "HuggingFace models failed to load"<a class="headerlink" href="#4-huggingface-models-failed-to-load" title="Permanent link">&para;</a></h5>
<p><strong>Problem:</strong> Local HuggingFace models won't load.</p>
<p><strong>Solutions:</strong><br />
- Ensure sufficient disk space (2-5GB per model)<br />
- Check internet connection (needed for initial download)<br />
- Try smaller models (edit <code>utils.py</code> to prioritize smaller models)<br />
- On Apple Silicon, ensure PyTorch with MPS is installed:<br />
<code>bash
  pip install torch --upgrade</code><br />
- Clear HuggingFace cache if corrupted:<br />
<code>bash
  rm -rf ~/.cache/huggingface/</code></p>
<h5 id="5-ollama-models-not-found">5. "Ollama models not found"<a class="headerlink" href="#5-ollama-models-not-found" title="Permanent link">&para;</a></h5>
<p><strong>Problem:</strong> Ollama models aren't detected.</p>
<p><strong>Solutions:</strong><br />
- Install Ollama: https://ollama.ai<br />
- Pull a model:<br />
<code>bash
  ollama pull llama2</code><br />
- Ensure Ollama service is running:<br />
<code>bash
  ollama serve
  # Or check if running:
  ps aux | grep ollama</code><br />
- List available models:<br />
<code>bash
  ollama list</code></p>
<h5 id="6-llamacpp-models-not-found">6. "llama.cpp models not found"<a class="headerlink" href="#6-llamacpp-models-not-found" title="Permanent link">&para;</a></h5>
<p><strong>Problem:</strong> llama.cpp can't find model files.</p>
<p><strong>Solutions:</strong><br />
- Install llama-cpp-python:<br />
<code>bash
  pip install llama-cpp-python</code><br />
- Set model path:<br />
<code>bash
  export LLAMA_CPP_MODEL_PATH="/path/to/model.gguf"</code><br />
- Or place models in default location:<br />
<code>bash
  mkdir -p ~/.cache/llama-cpp/
  # Copy your .gguf file there</code><br />
- Download GGUF models from HuggingFace (search for "gguf" models)</p>
<h5 id="7-import-errors-or-module-not-found">7. "Import errors" or "Module not found"<a class="headerlink" href="#7-import-errors-or-module-not-found" title="Permanent link">&para;</a></h5>
<p><strong>Problem:</strong> Python packages aren't installed.</p>
<p><strong>Solutions:</strong><br />
- Install requirements:<br />
<code>bash
  pip install -r requirements.txt</code><br />
- Use virtual environment (recommended):<br />
<code>bash
  python -m venv venv
  source venv/bin/activate  # On Windows: venv\Scripts\activate
  pip install -r requirements.txt</code><br />
- Check Python version (3.8+ required):<br />
<code>bash
  python --version</code></p>
<h5 id="8-streamlit-not-opening-in-browser">8. "Streamlit not opening in browser"<a class="headerlink" href="#8-streamlit-not-opening-in-browser" title="Permanent link">&para;</a></h5>
<p><strong>Problem:</strong> Streamlit interface doesn't open automatically.</p>
<p><strong>Solutions:</strong><br />
- Manually navigate to: <code>http://localhost:8501</code><br />
- Check if port 8501 is already in use:<br />
<code>bash
  lsof -i :8501</code><br />
- Use different port:<br />
<code>bash
  streamlit run rag_cli.py --server.port 8502</code></p>
<h5 id="9-slow-response-times">9. "Slow response times"<a class="headerlink" href="#9-slow-response-times" title="Permanent link">&para;</a></h5>
<p><strong>Problem:</strong> Queries take too long.</p>
<p><strong>Solutions:</strong><br />
- <strong>For local models:</strong> Use smaller models or cloud providers<br />
- <strong>For cloud providers:</strong> Check internet speed<br />
- <strong>Reduce retrieved documents:</strong> Edit <code>k</code> value in <code>rag_cli.py</code> (line 37)<br />
- <strong>Use GPU:</strong> Ensure Apple Silicon MPS or CUDA is available</p>
<h5 id="10-memory-errors-or-out-of-memory">10. "Memory errors" or "Out of memory"<a class="headerlink" href="#10-memory-errors-or-out-of-memory" title="Permanent link">&para;</a></h5>
<p><strong>Problem:</strong> System runs out of RAM.</p>
<p><strong>Solutions:</strong><br />
- Use smaller models (edit <code>utils.py</code> model list)<br />
- Reduce number of documents indexed<br />
- Use cloud providers instead of local models<br />
- Close other applications to free memory<br />
- Increase system RAM if possible</p>
<hr />
<h3 id="quick-reference">Quick Reference<a class="headerlink" href="#quick-reference" title="Permanent link">&para;</a></h3>
<h4 id="installation-checklist">Installation Checklist<a class="headerlink" href="#installation-checklist" title="Permanent link">&para;</a></h4>
<ul>
<li>[ ] Python 3.8+ installed</li>
<li>[ ] Virtual environment created and activated</li>
<li>[ ] Packages installed: <code>pip install -r requirements.txt</code></li>
<li>[ ] Optional: Ollama installed (for local LLMs)</li>
<li>[ ] Optional: llama-cpp-python installed (for GGUF models)</li>
</ul>
<h4 id="setup-checklist">Setup Checklist<a class="headerlink" href="#setup-checklist" title="Permanent link">&para;</a></h4>
<ul>
<li>[ ] <code>config.yaml</code> configured with correct directories</li>
<li>[ ] Documents placed in data directory</li>
<li>[ ] API keys set (optional, in <code>.env</code> file or environment)</li>
<li>[ ] Index created: <code>python main.py</code></li>
<li>[ ] Chatbot tested: <code>streamlit run rag_cli.py</code></li>
</ul>
<h4 id="common-commands">Common Commands<a class="headerlink" href="#common-commands" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="c1"># Create virtual environment</span>
python<span class="w"> </span>-m<span class="w"> </span>venv<span class="w"> </span>venv
<span class="nb">source</span><span class="w"> </span>venv/bin/activate<span class="w">  </span><span class="c1"># macOS/Linux</span>
venv<span class="se">\S</span>cripts<span class="se">\a</span>ctivate<span class="w">     </span><span class="c1"># Windows</span>

<span class="c1"># Install packages</span>
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt

<span class="c1"># Create index</span>
python<span class="w"> </span>main.py

<span class="c1"># Run chatbot</span>
streamlit<span class="w"> </span>run<span class="w"> </span>rag_cli.py

<span class="c1"># Check Ollama models</span>
ollama<span class="w"> </span>list

<span class="c1"># Pull Ollama model</span>
ollama<span class="w"> </span>pull<span class="w"> </span>llama2

<span class="c1"># Set environment variable</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OPENAI_API_KEY</span><span class="o">=</span><span class="s2">&quot;your-key&quot;</span>

<span class="c1"># Check if index exists</span>
ls<span class="w"> </span>-la<span class="w"> </span>rag-data/AI-Books/index/
</code></pre></div>

<h4 id="getting-api-keys_1">Getting API Keys<a class="headerlink" href="#getting-api-keys_1" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>OpenAI</strong>: https://platform.openai.com/api-keys</li>
<li><strong>Perplexity</strong>: https://www.perplexity.ai/settings/api</li>
<li><strong>Google Gemini</strong>: https://makersuite.google.com/app/apikey</li>
<li><strong>HuggingFace</strong>: https://huggingface.co/settings/tokens (optional)</li>
</ul>
<h4 id="local-model-requirements">Local Model Requirements<a class="headerlink" href="#local-model-requirements" title="Permanent link">&para;</a></h4>
<p><strong>For HuggingFace models:</strong><br />
- <strong>RAM</strong>: 4GB+ recommended for larger models<br />
- <strong>Disk Space</strong>: 2-5GB for model downloads<br />
- <strong>Internet</strong>: Required for initial model download<br />
- <strong>Apple Silicon</strong>: Automatically uses MPS (Metal Performance Shaders) for GPU acceleration</p>
<p><strong>For Ollama:</strong><br />
- <strong>RAM</strong>: 8GB+ recommended<br />
- <strong>Disk Space</strong>: 4-10GB per model<br />
- <strong>Installation</strong>: Download from https://ollama.ai<br />
- <strong>Models</strong>: Automatically downloaded on first use</p>
<p><strong>For llama.cpp:</strong><br />
- <strong>RAM</strong>: 4GB+ recommended<br />
- <strong>Disk Space</strong>: 2-8GB per model<br />
- <strong>Installation</strong>: <code>pip install llama-cpp-python</code><br />
- <strong>Models</strong>: Download GGUF format models from HuggingFace<br />
- <strong>Apple Silicon</strong>: Optimized with multi-threading support</p>
<h3 id="file-structure">File Structure<a class="headerlink" href="#file-structure" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>rag/
├── main.py              # Indexing script
├── rag_cli.py           # Streamlit chatbot
├── config.yaml          # Configuration
├── requirements.txt     # Dependencies
├── rag-data/           # Your documents
│   └── AI-Books/
│       ├── data/       # Source documents
│       └── index/      # Generated index
└── README.md           # This file
</code></pre></div>

<h3 id="performance-tips">Performance Tips<a class="headerlink" href="#performance-tips" title="Permanent link">&para;</a></h3>
<h4 id="for-local-mode">For Local Mode:<a class="headerlink" href="#for-local-mode" title="Permanent link">&para;</a></h4>
<ul>
<li>Use smaller models for faster inference</li>
<li>Consider using GPU acceleration if available</li>
<li>Monitor memory usage with larger models</li>
</ul>
<h4 id="for-api-mode">For API Mode:<a class="headerlink" href="#for-api-mode" title="Permanent link">&para;</a></h4>
<ul>
<li>Set up multiple API keys for redundancy</li>
<li>Monitor API quotas and costs</li>
<li>Use appropriate model sizes for your use case</li>
</ul>
        </div>
        
        <footer>
            <p><strong>RAG Course Chatbot Documentation</strong></p>
            <p class="timestamp">Last updated: 2026-01-04 15:18:09</p>
            <p class="timestamp">Generated from README.md</p>
        </footer>
    </div>
</body>
</html>