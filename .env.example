# .env.example - Comprehensive configuration template

# ============================================================================
# USER AGENT
# ============================================================================
USER_AGENT="rag-knowledge-graph/1.0 (your.email@example.com)"

# ============================================================================
# DATABASE CONFIGURATION
# ============================================================================
DATABASE_URL=postgresql://user:password@localhost:5432/rag_db
# Example: postgresql://raguser:ragpass123@localhost:5432/agentic_rag_db
VECTOR_DIMENSION=1536

# ============================================================================
# NEO4J CONFIGURATION (Knowledge Graph)
# ============================================================================
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_neo4j_password

# ============================================================================
# LLM PROVIDER CONFIGURATION
# ============================================================================
# Primary LLM Provider: ollama, openai, gemini, openrouter
LLM_PROVIDER=ollama

# Base URL for the LLM provider
# OpenAI: https://api.openai.com/v1
# Ollama: http://localhost:11434/v1
# OpenRouter: https://openrouter.ai/api/v1
# Gemini: https://generativelanguage.googleapis.com/v1beta
LLM_BASE_URL=http://localhost:11434/v1

# API Key for LLM provider
LLM_API_KEY=ollama

# The LLM model to use (must support tools/function calling)
# OpenAI: gpt-4o-mini, gpt-4.1-mini
# OpenRouter: anthropic/claude-3-5-sonnet
# Ollama: llama3.2:latest, qwen2.5:14b-instruct, DeepSeek-R1:14b
# Gemini: gemini-2.0-flash-exp, gemini-1.5-flash
LLM_CHOICE=llama3.2:latest

# Ingestion-specific LLM (can be different/faster model for processing)
# Leave empty to use the same as LLM_CHOICE
INGESTION_LLM_CHOICE=

# ============================================================================
# EMBEDDING PROVIDER CONFIGURATION
# ============================================================================
# Primary Embedding Provider: ollama, openai
EMBEDDING_PROVIDER=ollama

# Base URL for embedding models
# OpenAI: https://api.openai.com/v1
# Ollama: http://localhost:11434/v1
EMBEDDING_BASE_URL=http://localhost:11434/v1

# API Key for embedding provider
EMBEDDING_API_KEY=ollama

# The embedding model to use
# OpenAI: text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002
# Ollama: nomic-embed-text, mxbai-embed-large
EMBEDDING_MODEL=nomic-embed-text

# ============================================================================
# EXTERNAL FALLBACK PROVIDERS (when local unavailable)
# ============================================================================

# OpenAI Configuration
OPENAI_API_KEY=
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o-mini
OPENAI_EMBEDDING_MODEL=text-embedding-3-large

# Google Gemini Configuration
GOOGLE_API_KEY=
GEMINI_API_KEY=
GEMINI_MODEL=gemini-2.0-flash-exp

# Perplexity Configuration
PERPLEXITY_API_KEY=
PERPLEXITY_MODEL=mistral-7b-instruct

# OpenRouter Configuration
OPENROUTER_API_KEY=
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENROUTER_MODEL=anthropic/claude-3-5-sonnet

# ChatLLM Teams (RouteLLM) Configuration
CHATLLM_API_KEY=
CHATLLM_BASE_URL=https://api.chatllm.ai/v1
CHATLLM_MODEL=gpt-3.5-turbo

# HuggingFace Configuration (optional)
HUGGINGFACE_API_KEY=

# ============================================================================
# APPLICATION CONFIGURATION
# ============================================================================
APP_ENV=development
LOG_LEVEL=INFO
API_HOST=0.0.0.0
API_PORT=8000

# Debug Configuration
DEBUG_MODE=false
ENABLE_PROFILING=false

# ============================================================================
# DATA DIRECTORIES
# ============================================================================
DATA_DIR=./rag-data/data
INDEX_DIR=./rag-data/index
METADATA_DB=./rag-data/metadata.db

# ============================================================================
# PROCESSING CONFIGURATION
# ============================================================================

# Chunking Configuration (optimized for Graphiti token limits)
CHUNK_SIZE=800
CHUNK_OVERLAP=150
MAX_CHUNK_SIZE=1500

# Batch Processing
BATCH_SIZE=10
MAX_WORKERS=4

# Timeouts and Retries
REQUEST_TIMEOUT=30
MAX_RETRIES=3
RETRY_DELAY=1

# ============================================================================
# VECTOR SEARCH CONFIGURATION
# ============================================================================
MAX_SEARCH_RESULTS=10

# ============================================================================
# SESSION CONFIGURATION
# ============================================================================
SESSION_TIMEOUT_MINUTES=60
MAX_MESSAGES_PER_SESSION=100

# ============================================================================
# RATE LIMITING
# ============================================================================
RATE_LIMIT_REQUESTS=60
RATE_LIMIT_WINDOW_SECONDS=60

# ============================================================================
# FILE PROCESSING
# ============================================================================
MAX_FILE_SIZE_MB=10
ALLOWED_FILE_EXTENSIONS=.md,.txt,.pdf,.docx,.pptx,.xlsx,.html,.htm,.jpg,.jpeg,.png,.gif,.bmp,.tiff,.mp4,.avi,.mov,.mp3,.wav,.aac,.flac

# ============================================================================
# FEATURE FLAGS
# ============================================================================
ENABLE_GRAPH_BUILDING=true
ENABLE_WATCHDOG=false
OFFLINE_MODE=false

# ============================================================================
# OLLAMA SPECIFIC CONFIGURATION
# ============================================================================
# Comma-separated list of Ollama models to try in order
OLLAMA_MODELS=DeepSeek-R1:14b,qwen2.5-coder:14b,qwen2.5-coder:7b,llama3.2:latest,mistral
OLLAMA_TIMEOUT=60

# ============================================================================
# LLAMA.CPP CONFIGURATION (Optional)
# ============================================================================
# LLAMA_CPP_MODEL_PATH=/path/to/your/model.gguf
# LLAMA_CPP_N_CTX=2048

# ============================================================================
# HUGGINGFACE LOCAL MODELS (Optional)
# ============================================================================
# Comma-separated list of HuggingFace embedding models to try
HUGGINGFACE_EMBEDDING_MODELS=BAAI/bge-small-en-v1.5,sentence-transformers/all-MiniLM-L6-v2

# Comma-separated list of HuggingFace LLM models to try
HUGGINGFACE_LLM_MODELS=TinyLlama/TinyLlama-1.1B-Chat-v1.0,microsoft/DialoGPT-medium,gpt2,distilgpt2
HUGGINGFACE_MAX_LENGTH=2048
