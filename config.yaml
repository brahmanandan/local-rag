# config.yaml
DATA_DIR: "./rag-data/mesh/data"
INDEX_DIR: "./rag-data/mesh/index"

# Supported Providers and their API Key Environment Variables:
# - OpenAI: OPENAI_API_KEY (set in .env file or export as environment variable)
# - Perplexity: PERPLEXITY_API_KEY (set in .env file or export as environment variable)
# - Google Gemini: GOOGLE_API_KEY (set in .env file or export as environment variable)
# - HuggingFace: HUGGINGFACE_API_KEY (optional for some models)
# - Local Models: No API key required (HuggingFace models run locally)
#
# NOTE: API keys should NOT be stored in this file!
# Create a .env file in the project root with your API keys:
# OPENAI_API_KEY=your-key-here
# PERPLEXITY_API_KEY=your-key-here
# GOOGLE_API_KEY=your-key-here

# Embedding Model Priority (order matters - tried from top to bottom)
EMBEDDINGS_PRIORITY:
  - huggingface_bge # HuggingFace BAAI/bge-small-en-v1.5 (high quality)
  - huggingface     # HuggingFace sentence-transformers/all-MiniLM-L6-v2 (fallback)
  - openai          # OpenAI text-embedding-ada-002
  - openrouter      # OpenRouter OpenAI-compatible embeddings
  - chatllm         # ChatLLM OpenAI-compatible embeddings
  - perplexity      # Perplexity built-in embeddings
  - google          # Google Gemini embedding-001


# LLM Model Priority (order matters - tried from top to bottom)
# NOTE: Google (gemini-1.5-*) models currently have SDK compatibility issues.
# Use local Ollama/llama_cpp/HuggingFace instead, or upgrade langchain-google-genai.
LLM_PRIORITY:
  - ollama          # Ollama local models (llama2, mistral, llama3, phi3, gemma)
  - llama_cpp       # llama.cpp GGUF models
  - huggingface     # HuggingFace local models (phi-2, TinyLlama, etc.)
  - openai          # OpenAI GPT-3.5-turbo or GPT-4
  - openrouter      # OpenRouter OpenAI-compatible chat models
  - chatllm         # ChatLLM OpenAI-compatible chat models
  - perplexity      # Perplexity mistral-7b-instruct
  # - google          # (Disabled: SDK routing v1beta endpoint causes 404s for gemini-1.5-*)

# Model-specific configurations
MODELS:
  openai:
    embedding_model: "text-embedding-ada-002"
    llm_model: "gpt-3.5-turbo"
    temperature: 0.2

  openrouter:
    base_url: "https://openrouter.ai/api/v1"
    embedding_model: "text-embedding-3-small"
    llm_model: "gpt-3.5-turbo"
    temperature: 0.2

  chatllm:
    base_url: "https://api.chatllm.ai/v1"
    embedding_model: "text-embedding-3-small"
    llm_model: "gpt-3.5-turbo"
    temperature: 0.2

  perplexity:
    llm_model: "mistral-7b-instruct"
    temperature: 0.2

  google:
    embedding_model: "models/text-embedding-004"  # Prefer modern embedding model
    llm_model: "gemini-1.5-flash"  # Fallbacks handled in code if unavailable
    temperature: 0.2

  ollama:
    models:
      - "DeepSeek-R1:14b"     # Primary: Advanced reasoning model
      - "qwen2.5-coder:14b"   # Fallback: Code-focused model
      - "qwen2.5-coder:7b"    # Fallback: Smaller coder model
      - "llama2"              # Fallback: Legacy
      - "mistral"             # Fallback: Legacy
    temperature: 0.2
    timeout: 60  # Seconds to wait for Ollama responses

  llama_cpp:
    model_paths:  # Tried in order
      - "~/.cache/llama-cpp/llama-2-7b-chat.gguf"
      - "~/.cache/llama-cpp/mistral-7b-instruct-v0.1.gguf"
      - "~/.cache/llama-cpp/phi-3-mini-4k-instruct.gguf"
    n_ctx: 2048
    temperature: 0.2

  huggingface:
    embedding_models:  # Tried in order
      - "BAAI/bge-small-en-v1.5"
      - "sentence-transformers/all-MiniLM-L6-v2"
    llm_models:  # Tried in order (phi-2 removed due to tensor issues)
      - "TinyLlama/TinyLlama-1.1B-Chat-v1.0"  # Best for RAG, chat-optimized
      - "microsoft/DialoGPT-medium"  # Good for conversations
      - "gpt2"  # Reliable fallback
      - "distilgpt2"  # Smallest fallback
    max_length: 2048  # Maximum sequence length
    temperature: 0.2
